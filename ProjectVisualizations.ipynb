{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CASES_PATH = \"results_remote_lb\"\n",
    "\n",
    "TEST_CASES_DICT = {\n",
    "    'Baseline': '2019_04_04_161815',\n",
    "    'Default Scaling': '2019_04_04_203319',\n",
    "    'Low Threshold Scaling': '2019_04_04_233925',\n",
    "    'Aggressive Scaling': '2019_04_05_145551'\n",
    "}\n",
    "\n",
    "FILE_TYPE_AWS_METRICS = 'aws_metrics_{0}_*.csv'\n",
    "FILE_TYPE_USER_RESPONSE = 'users{1}_response_{0}.csv'\n",
    "\n",
    "SAMPLE_PERIOD_SECONDS = 30\n",
    "\n",
    "def read_file(test_case_name, file_type, *args):\n",
    "    file_name = file_type.format(*args)\n",
    "    file_search_path = os.path.join(TEST_CASES_PATH, TEST_CASES_DICT[test_case_name], file_name)\n",
    "    file_paths = glob.glob(file_search_path)\n",
    "    \n",
    "    # Get test file with test params if not searching aws metrics\n",
    "    if file_type != FILE_TYPE_AWS_METRICS:\n",
    "        s_file_name = FILE_TYPE_AWS_METRICS.format(args[0])\n",
    "        s_file_search_path = os.path.join(TEST_CASES_PATH, TEST_CASES_DICT[test_case_name], s_file_name)\n",
    "        s_file_paths = glob.glob(s_file_search_path)\n",
    "        info_file = s_file_paths[0]\n",
    "    else:\n",
    "        info_file = file_paths[0]\n",
    "        \n",
    "    params_list = os.path.basename(info_file)[12:].split('_')\n",
    "    \n",
    "    params_dict = {\n",
    "        'start_time_seconds': int(params_list[1]),\n",
    "        'end_time_seconds': int(params_list[2])\n",
    "    }\n",
    "\n",
    "    return pd.read_csv(file_paths[0]), params_dict\n",
    "\n",
    "def get_aws_metrics_for_test_case(test_case_name, metrics='cpu0_util'):\n",
    "    aws_metrics_list = [read_file(test_case_name, FILE_TYPE_AWS_METRICS, i) for i in range(3)]\n",
    "    aws_metrics_list = [aws_metrics[0].pivot_table(metrics, ['timepoint'], 'instance_id')\n",
    "                                for aws_metrics in aws_metrics_list]\n",
    "    aws_metrics_list = [aws_metrics[aws_metrics.index.astype(int) < int(1800 / SAMPLE_PERIOD_SECONDS)]\n",
    "                        for aws_metrics in aws_metrics_list]\n",
    "    return aws_metrics_list\n",
    "\n",
    "\n",
    "def get_user_responses_for_test_case(test_case_name, user='A'):\n",
    "    user_responses_list = []\n",
    "    for i in range(3):\n",
    "        df, params_dict = read_file(test_case_name, FILE_TYPE_USER_RESPONSE, i, user)\n",
    "        df.loc[:,'timepoint'] = (df.loc[:,'timeStamp'] / 1000) - params_dict['start_time_seconds']\n",
    "        user_responses_list.append(df)\n",
    "    return user_responses_list\n",
    "\n",
    "def init_figure(figsize, xlabel, ylabel, ylim):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim(ylim)\n",
    "\n",
    "def plot_cpu0_util_for_test_case(test_case_name, case_num='all', show='all', new_figure=True):\n",
    "    \n",
    "    if new_figure:\n",
    "        init_figure(figsize=(10,10), xlabel='Time (Minutes)', ylabel='CPU Utilization', ylim=(0,1))\n",
    "\n",
    "    aws_metrics_list_pivoted = get_aws_metrics_for_test_case(test_case_name)\n",
    "    if show != 'avg' and case_num == 'all':\n",
    "        for i in range(3):\n",
    "            aws_metrics = aws_metrics_list_pivoted[i]\n",
    "            x = np.arange(0, len(aws_metrics) * SAMPLE_PERIOD_SECONDS / 60, SAMPLE_PERIOD_SECONDS / 60)\n",
    "            plt.plot(x, aws_metrics)\n",
    "    if show != 'each' or case_num != 'all':\n",
    "        if show != 'each':\n",
    "            df_concat = pd.concat(aws_metrics_list_pivoted, sort=False)\n",
    "            by_row_index = df_concat.groupby(df_concat.index)\n",
    "            df_means = by_row_index.mean()\n",
    "            aws_metrics = df_means.apply(lambda row: row[row!=0].dropna().mean(), axis=1)\n",
    "        else:\n",
    "            aws_metrics = aws_metrics_list_pivoted[case_num]\n",
    "            \n",
    "        x = np.arange(0, len(aws_metrics) * SAMPLE_PERIOD_SECONDS / 60, SAMPLE_PERIOD_SECONDS / 60)\n",
    "        lines = plt.plot(x, aws_metrics, label='{} Avg'.format(test_case_name))\n",
    "        lines[0].set_dashes([2, 2, 10, 2])\n",
    "    plt.legend(fontsize=16)\n",
    "\n",
    "def plot_user_response_time_for_test_case(test_case_name, case_num='all', user='A', show='all', new_figure=True):\n",
    "    \"\"\"\n",
    "    :param: case_num\n",
    "    \"\"\"\n",
    "    if new_figure:\n",
    "        init_figure(figsize=(10,10), xlabel='Time (Minutes)', ylabel='Response Time', ylim=(0,42000))\n",
    "\n",
    "    user_responses_list = get_user_responses_for_test_case(test_case_name, user)\n",
    "    if show != 'avg' and case_num == 'all':\n",
    "        shared_min = min([df.loc[:,'timepoint'].min() for df in user_responses_list])\n",
    "        shared_max = max([df.loc[:,'timepoint'].max() for df in user_responses_list])\n",
    "\n",
    "        bins = np.linspace(shared_min, shared_max, num=50)\n",
    "        for df in user_responses_list:\n",
    "            df['binnedTimepoint'] = pd.cut(df['timepoint'], bins)\n",
    "            binned_df = df.groupby('binnedTimepoint').mean()\n",
    "            binned_df = binned_df[binned_df['timepoint'] / 60 <= 30]\n",
    "            plt.plot(binned_df['timepoint'] / 60, binned_df.loc[:,'elapsed'].values)\n",
    "    if show != 'each' or case_num != 'all':\n",
    "        if show != 'each':\n",
    "            df = pd.concat(user_responses_list).sort_values(by='timepoint').reset_index(drop=True)\n",
    "        else:\n",
    "            df = user_responses_list[case_num]\n",
    "        \n",
    "        bins = np.linspace(df.loc[:,'timepoint'].min(), df.loc[:,'timepoint'].max(), num=50)\n",
    "        df['binnedTimepoint'] = pd.cut(df['timepoint'], bins)\n",
    "        binned_df = df.groupby('binnedTimepoint').mean()\n",
    "        binned_df = binned_df[binned_df['timepoint'] / 60 <= 30]\n",
    "\n",
    "        lines = plt.plot(binned_df['timepoint'] / 60, binned_df.loc[:,'elapsed'].values,\n",
    "                         label='{} Avg'.format(test_case_name))\n",
    "        lines[0].set_dashes([2, 2, 10, 2])\n",
    "    plt.legend(fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test plot functions and params below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cpu0_util_for_test_case('Baseline', case_num='all', show='all', new_figure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cpu0_util_for_test_case('Default Scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cpu0_util_for_test_case('Low Threshold Scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cpu0_util_for_test_case('Aggressive Scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_user_response_time_for_test_case('Baseline', case_num='all', user='A', show='all', new_figure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_user_response_time_for_test_case('Default Scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_user_response_time_for_test_case('Low Threshold Scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_user_response_time_for_test_case('Aggressive Scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below plot functions used in Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_user_response_time_for_test_case('Baseline', user='A', show='avg', new_figure=True)\n",
    "plot_user_response_time_for_test_case('Default Scaling', user='A', show='avg', new_figure=False)\n",
    "plot_user_response_time_for_test_case('Low Threshold Scaling', user='A', show='avg', new_figure=False)\n",
    "plot_user_response_time_for_test_case('Aggressive Scaling', user='A', show='avg', new_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_user_response_time_for_test_case('Baseline', user='B', show='avg', new_figure=True)\n",
    "plot_user_response_time_for_test_case('Default Scaling', user='B', show='avg', new_figure=False)\n",
    "plot_user_response_time_for_test_case('Low Threshold Scaling', user='B', show='avg', new_figure=False)\n",
    "plot_user_response_time_for_test_case('Aggressive Scaling', user='B', show='avg', new_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_user_response_time_for_test_case('Baseline', user='C', show='avg', new_figure=True)\n",
    "plot_user_response_time_for_test_case('Default Scaling', user='C', show='avg', new_figure=False)\n",
    "plot_user_response_time_for_test_case('Low Threshold Scaling', user='C', show='avg', new_figure=False)\n",
    "plot_user_response_time_for_test_case('Aggressive Scaling', user='C', show='avg', new_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cpu0_util_for_test_case('Baseline', show='avg')\n",
    "plot_cpu0_util_for_test_case('Default Scaling', show='avg', new_figure=False)\n",
    "plot_cpu0_util_for_test_case('Low Threshold Scaling', show='avg', new_figure=False)\n",
    "plot_cpu0_util_for_test_case('Aggressive Scaling', show='avg', new_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Cost Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_case_costs = []\n",
    "avg_test_case_labels = []\n",
    "\n",
    "for test_case_name in TEST_CASES_DICT.keys():\n",
    "    running_time_list = get_aws_metrics_for_test_case(test_case_name, metrics='running_time_ms')\n",
    "    avg_test_case_cost = np.mean([0.1 / 3600 * sum(running_time_series.iloc[0,:]) for running_time_series in running_time_list])\n",
    "    avg_test_case_costs.append(avg_test_case_cost)\n",
    "    avg_test_case_labels.append(test_case_name)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlabel('Test Case', fontsize=16)\n",
    "plt.ylabel('Avg Cost Over Test Duration ($)', fontsize=16)\n",
    "\n",
    "plt.bar(range(len(avg_test_case_costs)), avg_test_case_costs, tick_label=avg_test_case_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
